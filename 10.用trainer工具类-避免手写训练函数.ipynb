{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0db0cb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kennywu/opt/anaconda3/envs/dl/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 2.91kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 145kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 466kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 650kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#加载分词工具\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c941b73",
   "metadata": {},
   "source": [
    "AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b6b4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/kennywu/Documents/Github_repos/Huggingface_CN_Toturials/data/glue_sst2/train/cache-d5a332e72039e5e9_*_of_00004.arrow\n",
      "Loading cached processed dataset at /Users/kennywu/Documents/Github_repos/Huggingface_CN_Toturials/data/glue_sst2/validation/cache-54ebd1d2495e5e86_*_of_00004.arrow\n",
      "Loading cached processed dataset at /Users/kennywu/Documents/Github_repos/Huggingface_CN_Toturials/data/glue_sst2/test/cache-47c71c904c75dba8_*_of_00004.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "#加载数据集\n",
    "#从网络加载\n",
    "#datasets = load_dataset(path='glue', name='sst2')\n",
    "\n",
    "#从本地磁盘加载数据\n",
    "datasets = load_from_disk('./data/glue_sst2')\n",
    "\n",
    "\n",
    "#分词\n",
    "def f(data):\n",
    "    return tokenizer(\n",
    "        data['sentence'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=30,\n",
    "    )\n",
    "\n",
    "\n",
    "datasets = datasets.map(f, batched=True, batch_size=1000, num_proc=4)\n",
    "\n",
    "#取数据子集，否则数据太多跑不动\n",
    "dataset_train = datasets['train'].shuffle().select(range(1000))\n",
    "dataset_test = datasets['validation'].shuffle().select(range(200))\n",
    "\n",
    "del datasets\n",
    "\n",
    "dataset_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6ce4d24",
   "metadata": {},
   "source": [
    "用AutoModelForSequenceClassification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74b91f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10831.181\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "#加载模型 AutoModelForSequenceClassification，指定为2分类\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased',\n",
    "                                                           num_labels=2)\n",
    "\n",
    "print(sum([i.nelement() for i in model.parameters()]) / 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "876c0287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w9/g6sxmrzn3bj5clmc2jx9vzt40000gn/T/ipykernel_53185/623941755.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('accuracy')\n",
      "Downloading builder script: 4.21kB [00:00, 1.06MB/s]                   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 1.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "\n",
    "#加载评价函数\n",
    "#有时会因为网络问题卡主,反复尝试会成功的\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "\n",
    "#定义评价函数\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    logits = logits.argmax(axis=1)\n",
    "    return metric.compute(predictions=logits, references=labels)\n",
    "\n",
    "\n",
    "#模拟测试输出\n",
    "eval_pred = EvalPrediction(\n",
    "    predictions=np.array([[0, 1], [2, 3], [4, 5], [6, 7]]),\n",
    "    label_ids=np.array([1, 1, 1, 1]),\n",
    ")\n",
    "\n",
    "compute_metrics(eval_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7e6b295",
   "metadata": {},
   "source": [
    "# transformers的 TrainingArguments, Trainer\n",
    "transformers库中用于训练模型的两个重要类。\n",
    "\n",
    "## TrainingArguments是一个用于设置训练参数的类，\n",
    "包含了许多用于控制训练过程的参数，例如训练时的学习率、batch size、epoch数、优化器类型等等。这些参数可以通过实例化TrainingArguments类并传递相应的参数来设置。\n",
    "\n",
    "## Trainer是一个用于训练模型的类，\n",
    "它可以接受一个模型和一个数据集，并使用指定的训练参数进行训练。\n",
    "在训练过程中，Trainer类可以自动进行数据加载、优化器设置、模型评估、日志记录等操作。Trainer类还可以使用多个GPU进行分布式训练。\n",
    "\n",
    "核心：可以避免手动编写训练循环（training loop）的代码，不需要再写def train函数了。Trainer类已经实现了完整的训练过程，包括数据加载、前向传播、反向传播、梯度更新、模型评估等步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2c59b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "#初始化训练参数\n",
    "args = TrainingArguments(output_dir='./output_dir', # 模型参数保存地址\n",
    "                         evaluation_strategy='epoch',\n",
    "                         no_cuda=True)\n",
    "# 些参数可以通过实例化TrainingArguments类并传递相应的参数来设置。\n",
    "args.num_train_epochs = 1\n",
    "args.learning_rate = 1e-4\n",
    "args.weight_decay = 1e-2\n",
    "args.per_device_eval_batch_size = 32\n",
    "args.per_device_train_batch_size = 16\n",
    "\n",
    "#初始化训练器： 在这里定义\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset_train, # Trainer的好处，连dataset都不用定义了！\n",
    "    eval_dataset=dataset_test, # eval data也有好处\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd5b54aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100%|██████████| 7/7 [00:11<00:00,  1.62s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4577715992927551,\n",
       " 'eval_accuracy': 0.81,\n",
       " 'eval_runtime': 13.5759,\n",
       " 'eval_samples_per_second': 14.732,\n",
       " 'eval_steps_per_second': 0.516}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 不训练，直接测试初始模型\n",
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1aedbbd4",
   "metadata": {},
   "source": [
    "可以避免手动编写训练循环（training loop）的代码，不需要再写def train函数了。Trainer类已经实现了完整的训练过程，包括数据加载、前向传播、反向传播、梯度更新、模型评估等步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "104de6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/Users/kennywu/opt/anaconda3/envs/dl/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 63\n",
      "  Number of trainable parameters = 108311810\n",
      "100%|██████████| 63/63 [04:41<00:00,  3.88s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "                                               \n",
      "100%|██████████| 63/63 [04:55<00:00,  3.88s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 63/63 [04:55<00:00,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4577715992927551, 'eval_accuracy': 0.81, 'eval_runtime': 14.0756, 'eval_samples_per_second': 14.209, 'eval_steps_per_second': 0.497, 'epoch': 1.0}\n",
      "{'train_runtime': 295.1052, 'train_samples_per_second': 3.389, 'train_steps_per_second': 0.213, 'train_loss': 0.629571278889974, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63, training_loss=0.629571278889974, metrics={'train_runtime': 295.1052, 'train_samples_per_second': 3.389, 'train_steps_per_second': 0.213, 'train_loss': 0.629571278889974, 'epoch': 1.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#训练 1个epoch\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6630698b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100%|██████████| 7/7 [00:11<00:00,  1.65s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4577715992927551,\n",
       " 'eval_accuracy': 0.81,\n",
       " 'eval_runtime': 13.7887,\n",
       " 'eval_samples_per_second': 14.505,\n",
       " 'eval_steps_per_second': 0.508}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 再次评价模型\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40bddaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output_dir\n",
      "Configuration saved in ./output_dir/config.json\n",
      "Model weights saved in ./output_dir/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "#保存模型的【参数】，不是模型本身\n",
    "trainer.save_model(output_dir='./output_dir')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d123170",
   "metadata": {},
   "source": [
    "# 重新加在保存的模型，来预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ed86d9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    label = [i['label'] for i in data]\n",
    "    input_ids = [i['input_ids'] for i in data]\n",
    "    token_type_ids = [i['token_type_ids'] for i in data]\n",
    "    attention_mask = [i['attention_mask'] for i in data]\n",
    "\n",
    "    label = torch.LongTensor(label)\n",
    "    input_ids = torch.LongTensor(input_ids)\n",
    "    token_type_ids = torch.LongTensor(token_type_ids)\n",
    "    attention_mask = torch.LongTensor(attention_mask)\n",
    "\n",
    "    return label, input_ids, token_type_ids, attention_mask\n",
    "\n",
    "\n",
    "#单独 test数据加载器:  因为不是trainer ，没有办法，只能用torch dataset\n",
    "loader_test = torch.utils.data.DataLoader(dataset=dataset_test, # 还是上面的validation\n",
    "                                          batch_size=4,\n",
    "                                          collate_fn=collate_fn,\n",
    "                                        #   shuffle=True,\n",
    "                                            shuffle=False,\n",
    "                                          drop_last=True)\n",
    "\n",
    "for i, (label, input_ids, token_type_ids,\n",
    "        attention_mask) in enumerate(loader_test):\n",
    "    break\n",
    "\n",
    "# label, input_ids, token_type_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19fd71e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 测试\n",
    "def test():\n",
    "    #加载参数\n",
    "    model.load_state_dict(torch.load('./output_dir/pytorch_model.bin'))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    #运算\n",
    "    out = model(input_ids=input_ids,\n",
    "                token_type_ids=token_type_ids,\n",
    "                attention_mask=attention_mask)\n",
    "\n",
    "    #[4, 2] -> [4]\n",
    "    out = out['logits'].argmax(dim=1)\n",
    "\n",
    "    correct = (out == label).sum().item()\n",
    "\n",
    "    return correct / len(label)\n",
    "\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
